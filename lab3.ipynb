{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: WordPiece tokenization splits words into smaller units for efficient encoding.\n",
      "Unigrams: [('wordpiece',), ('tokenization',), ('splits',), ('words',), ('into',), ('smaller',), ('units',), ('for',), ('efficient',), ('encoding',), ('.',)]\n",
      "Bigrams: [('wordpiece', 'tokenization'), ('tokenization', 'splits'), ('splits', 'words'), ('words', 'into'), ('into', 'smaller'), ('smaller', 'units'), ('units', 'for'), ('for', 'efficient'), ('efficient', 'encoding'), ('encoding', '.')]\n",
      "Trigrams: [('wordpiece', 'tokenization', 'splits'), ('tokenization', 'splits', 'words'), ('splits', 'words', 'into'), ('words', 'into', 'smaller'), ('into', 'smaller', 'units'), ('smaller', 'units', 'for'), ('units', 'for', 'efficient'), ('for', 'efficient', 'encoding'), ('efficient', 'encoding', '.')]\n",
      "\n",
      "Sentence 2: Natural Language Processing enables machines to understand human language.\n",
      "Unigrams: [('natural',), ('language',), ('processing',), ('enables',), ('machines',), ('to',), ('understand',), ('human',), ('language',), ('.',)]\n",
      "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'enables'), ('enables', 'machines'), ('machines', 'to'), ('to', 'understand'), ('understand', 'human'), ('human', 'language'), ('language', '.')]\n",
      "Trigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'enables'), ('processing', 'enables', 'machines'), ('enables', 'machines', 'to'), ('machines', 'to', 'understand'), ('to', 'understand', 'human'), ('understand', 'human', 'language'), ('human', 'language', '.')]\n",
      "\n",
      "Sentence 3: Embedding layers convert words into dense vector representations.\n",
      "Unigrams: [('embedding',), ('layers',), ('convert',), ('words',), ('into',), ('dense',), ('vector',), ('representations',), ('.',)]\n",
      "Bigrams: [('embedding', 'layers'), ('layers', 'convert'), ('convert', 'words'), ('words', 'into'), ('into', 'dense'), ('dense', 'vector'), ('vector', 'representations'), ('representations', '.')]\n",
      "Trigrams: [('embedding', 'layers', 'convert'), ('layers', 'convert', 'words'), ('convert', 'words', 'into'), ('words', 'into', 'dense'), ('into', 'dense', 'vector'), ('dense', 'vector', 'representations'), ('vector', 'representations', '.')]\n",
      "\n",
      "TF-IDF Scores:\n",
      "\n",
      "Document 1:\n",
      "efficient: 0.1971\n",
      "efficient encoding: 0.1971\n",
      "encoding: 0.1971\n",
      "for: 0.1971\n",
      "for efficient: 0.1971\n",
      "for efficient encoding: 0.1971\n",
      "into smaller: 0.1971\n",
      "into smaller units: 0.1971\n",
      "smaller: 0.1971\n",
      "smaller units: 0.1971\n",
      "smaller units for: 0.1971\n",
      "splits: 0.1971\n",
      "splits words: 0.1971\n",
      "splits words into: 0.1971\n",
      "tokenization: 0.1971\n",
      "tokenization splits: 0.1971\n",
      "tokenization splits words: 0.1971\n",
      "units: 0.1971\n",
      "units for: 0.1971\n",
      "units for efficient: 0.1971\n",
      "wordpiece: 0.1971\n",
      "wordpiece tokenization: 0.1971\n",
      "wordpiece tokenization splits: 0.1971\n",
      "words into smaller: 0.1971\n",
      "into: 0.1499\n",
      "words: 0.1499\n",
      "words into: 0.1499\n",
      "\n",
      "Document 2:\n",
      "language: 0.3922\n",
      "enables: 0.1961\n",
      "enables machines: 0.1961\n",
      "enables machines to: 0.1961\n",
      "human: 0.1961\n",
      "human language: 0.1961\n",
      "language processing: 0.1961\n",
      "language processing enables: 0.1961\n",
      "machines: 0.1961\n",
      "machines to: 0.1961\n",
      "machines to understand: 0.1961\n",
      "natural: 0.1961\n",
      "natural language: 0.1961\n",
      "natural language processing: 0.1961\n",
      "processing: 0.1961\n",
      "processing enables: 0.1961\n",
      "processing enables machines: 0.1961\n",
      "to: 0.1961\n",
      "to understand: 0.1961\n",
      "to understand human: 0.1961\n",
      "understand: 0.1961\n",
      "understand human: 0.1961\n",
      "understand human language: 0.1961\n",
      "\n",
      "Document 3:\n",
      "convert: 0.2251\n",
      "convert words: 0.2251\n",
      "convert words into: 0.2251\n",
      "dense: 0.2251\n",
      "dense vector: 0.2251\n",
      "dense vector representations: 0.2251\n",
      "embedding: 0.2251\n",
      "embedding layers: 0.2251\n",
      "embedding layers convert: 0.2251\n",
      "into dense: 0.2251\n",
      "into dense vector: 0.2251\n",
      "layers: 0.2251\n",
      "layers convert: 0.2251\n",
      "layers convert words: 0.2251\n",
      "representations: 0.2251\n",
      "vector: 0.2251\n",
      "vector representations: 0.2251\n",
      "words into dense: 0.2251\n",
      "into: 0.1712\n",
      "words: 0.1712\n",
      "words into: 0.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "corpus = [\n",
    "    \"WordPiece tokenization splits words into smaller units for efficient encoding.\",\n",
    "    \"Natural Language Processing enables machines to understand human language.\",\n",
    "    \"Embedding layers convert words into dense vector representations.\"\n",
    "]\n",
    "\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    print(f\"\\nSentence {i+1}: {text}\")\n",
    "    print(\"Unigrams:\", generate_ngrams(text, 1))\n",
    "    print(\"Bigrams:\", generate_ngrams(text, 2))\n",
    "    print(\"Trigrams:\", generate_ngrams(text, 3))\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))  \n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"\\nTF-IDF Scores:\")\n",
    "for i, doc in enumerate(tfidf_scores):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    \n",
    "    scored_terms = [(j, score) for j, score in enumerate(doc) if score > 0]\n",
    "    \n",
    "    scored_terms_sorted = sorted(scored_terms, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for j, score in scored_terms_sorted:\n",
    "        print(f\"{feature_names[j]}: {score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: WordPiece tokenization splits words into smaller units for efficient encoding.\n",
      "N-grams (1-3): [('wordpiece',), ('tokenization',), ('splits',), ('words',), ('into',), ('smaller',), ('units',), ('for',), ('efficient',), ('encoding',), ('wordpiece', 'tokenization'), ('tokenization', 'splits'), ('splits', 'words'), ('words', 'into'), ('into', 'smaller'), ('smaller', 'units'), ('units', 'for'), ('for', 'efficient'), ('efficient', 'encoding'), ('wordpiece', 'tokenization', 'splits'), ('tokenization', 'splits', 'words'), ('splits', 'words', 'into'), ('words', 'into', 'smaller'), ('into', 'smaller', 'units'), ('smaller', 'units', 'for'), ('units', 'for', 'efficient'), ('for', 'efficient', 'encoding')]\n",
      "\n",
      "Sentence 2: Natural Language Processing enables machines to understand human language.\n",
      "N-grams (1-3): [('natural',), ('language',), ('processing',), ('enables',), ('machines',), ('to',), ('understand',), ('human',), ('language',), ('natural', 'language'), ('language', 'processing'), ('processing', 'enables'), ('enables', 'machines'), ('machines', 'to'), ('to', 'understand'), ('understand', 'human'), ('human', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'enables'), ('processing', 'enables', 'machines'), ('enables', 'machines', 'to'), ('machines', 'to', 'understand'), ('to', 'understand', 'human'), ('understand', 'human', 'language')]\n",
      "\n",
      "Sentence 3: Embedding layers convert words into dense vector representations.\n",
      "N-grams (1-3): [('embedding',), ('layers',), ('convert',), ('words',), ('into',), ('dense',), ('vector',), ('representations',), ('embedding', 'layers'), ('layers', 'convert'), ('convert', 'words'), ('words', 'into'), ('into', 'dense'), ('dense', 'vector'), ('vector', 'representations'), ('embedding', 'layers', 'convert'), ('layers', 'convert', 'words'), ('convert', 'words', 'into'), ('words', 'into', 'dense'), ('into', 'dense', 'vector'), ('dense', 'vector', 'representations')]\n",
      "\n",
      "TF-IDF Scores (Aligned with sklearn):\n",
      "\n",
      "Document 1:\n",
      "wordpiece: 0.1971\n",
      "tokenization: 0.1971\n",
      "splits: 0.1971\n",
      "smaller: 0.1971\n",
      "units: 0.1971\n",
      "for: 0.1971\n",
      "efficient: 0.1971\n",
      "encoding: 0.1971\n",
      "wordpiece tokenization: 0.1971\n",
      "tokenization splits: 0.1971\n",
      "splits words: 0.1971\n",
      "into smaller: 0.1971\n",
      "smaller units: 0.1971\n",
      "units for: 0.1971\n",
      "for efficient: 0.1971\n",
      "efficient encoding: 0.1971\n",
      "wordpiece tokenization splits: 0.1971\n",
      "tokenization splits words: 0.1971\n",
      "splits words into: 0.1971\n",
      "words into smaller: 0.1971\n",
      "into smaller units: 0.1971\n",
      "smaller units for: 0.1971\n",
      "units for efficient: 0.1971\n",
      "for efficient encoding: 0.1971\n",
      "words: 0.1499\n",
      "into: 0.1499\n",
      "words into: 0.1499\n",
      "\n",
      "Document 2:\n",
      "language: 0.3922\n",
      "natural: 0.1961\n",
      "processing: 0.1961\n",
      "enables: 0.1961\n",
      "machines: 0.1961\n",
      "to: 0.1961\n",
      "understand: 0.1961\n",
      "human: 0.1961\n",
      "natural language: 0.1961\n",
      "language processing: 0.1961\n",
      "processing enables: 0.1961\n",
      "enables machines: 0.1961\n",
      "machines to: 0.1961\n",
      "to understand: 0.1961\n",
      "understand human: 0.1961\n",
      "human language: 0.1961\n",
      "natural language processing: 0.1961\n",
      "language processing enables: 0.1961\n",
      "processing enables machines: 0.1961\n",
      "enables machines to: 0.1961\n",
      "machines to understand: 0.1961\n",
      "to understand human: 0.1961\n",
      "understand human language: 0.1961\n",
      "\n",
      "Document 3:\n",
      "embedding: 0.2251\n",
      "layers: 0.2251\n",
      "convert: 0.2251\n",
      "dense: 0.2251\n",
      "vector: 0.2251\n",
      "representations: 0.2251\n",
      "embedding layers: 0.2251\n",
      "layers convert: 0.2251\n",
      "convert words: 0.2251\n",
      "into dense: 0.2251\n",
      "dense vector: 0.2251\n",
      "vector representations: 0.2251\n",
      "embedding layers convert: 0.2251\n",
      "layers convert words: 0.2251\n",
      "convert words into: 0.2251\n",
      "words into dense: 0.2251\n",
      "into dense vector: 0.2251\n",
      "dense vector representations: 0.2251\n",
      "words: 0.1712\n",
      "into: 0.1712\n",
      "words into: 0.1712\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "corpus = [\n",
    "    \"WordPiece tokenization splits words into smaller units for efficient encoding.\",\n",
    "    \"Natural Language Processing enables machines to understand human language.\",\n",
    "    \"Embedding layers convert words into dense vector representations.\"\n",
    "]\n",
    "\n",
    "\n",
    "def tokenize_and_ngram(text, ngram_range=(1, 3)):\n",
    "    \"\"\"Tokenize text and generate n-grams (1-3 grams).\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    ngrams_list = []\n",
    "    for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        ngrams_list.extend([tuple(words[i:i+n]) for i in range(len(words) - n + 1)])\n",
    "    return ngrams_list\n",
    "\n",
    "def compute_tf(doc_ngrams):\n",
    "    tf = defaultdict(int)\n",
    "    for ngram in doc_ngrams:\n",
    "        tf[ngram] += 1\n",
    "    return tf\n",
    "\n",
    "def compute_idf(corpus_ngrams):\n",
    "    idf = defaultdict(float)\n",
    "    total_docs = len(corpus_ngrams)\n",
    "    for doc in corpus_ngrams:\n",
    "        unique_ngrams = set(doc)\n",
    "        for ngram in unique_ngrams:\n",
    "            idf[ngram] += 1\n",
    "    for ngram, df in idf.items():\n",
    "        idf[ngram] = math.log((1 + total_docs) / (1 + df)) + 1  \n",
    "    return idf\n",
    "\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "    corpus_ngrams = [tokenize_and_ngram(doc) for doc in corpus]\n",
    "    \n",
    "\n",
    "    idf = compute_idf(corpus_ngrams)\n",
    "    \n",
    "   \n",
    "    tfidf_corpus = []\n",
    "    for doc_ngrams in corpus_ngrams:\n",
    "        tf = compute_tf(doc_ngrams)\n",
    "        tfidf = defaultdict(float)\n",
    "        for ngram, count in tf.items():\n",
    "            tfidf[ngram] = count * idf[ngram]  # TF * IDF\n",
    "        \n",
    "        norm = math.sqrt(sum(score ** 2 for score in tfidf.values()))\n",
    "        if norm != 0:\n",
    "            for ngram in tfidf:\n",
    "                tfidf[ngram] /= norm\n",
    "        tfidf_corpus.append(tfidf)\n",
    "    return tfidf_corpus, idf\n",
    "\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    ngrams = tokenize_and_ngram(text)\n",
    "    print(f\"\\nSentence {i+1}: {text}\")\n",
    "    print(\"N-grams (1-3):\", ngrams)\n",
    "\n",
    "tfidf_corpus, idf = compute_tfidf(corpus)\n",
    "print(\"\\nTF-IDF Scores (Aligned with sklearn):\")\n",
    "for i, tfidf in enumerate(tfidf_corpus):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    for ngram, score in sorted(tfidf.items(), key=lambda x: -x[1]):\n",
    "        if score > 0:\n",
    "            print(f\"{' '.join(ngram)}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
